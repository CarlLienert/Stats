---
title: "Exam_One_A"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```





# Terminology
1. What is the difference between a *population* and a *sample*?

Answer: 


2. What is the difference between a *statistic* and a *parameter*.  Give an example of a *statistic*.

Answer:


3. In a data set, what is the difference between an *observation* and a *variable*?

Answer:



# Exploratory Data Analysis

Use one of the following two chunks to load the data set `Alelager`.

If you have the `resampledata` package installed use:

```{r}
library(resampledata)
BeerData<-Alelager
head(BeerData)
```

**or**, you can use:

```{r}
BeerData<-read.csv("https://sites.google.com/site/chiharahesterberg/data2/Alelager.csv")
head(BeerData)
```

1. Create side-by-side box plots of the alcohol content, grouped by beer type. Comment on the information the box plots convey.

```{r}

```

Commentary: 


2. Create a histogram of the calorie content of all beers in the data set. Describe the distribution.
```{r}

```

Description of distribution:

3.  Sampling Distribution
*	Simulate the sampling distribution of the mean of the number of calories, for samples of size 3.
		
```{r}

```



*	Create a histogram and describe the simulated sampling distribution.
		
```{r}

```


Description of sampling distribution:		
		
*	Use the simulation to find the probability that the average calorie content of 3 beers is less than 150 calories.
		
```{r}
```
		
* Use the simulation to find the probability that the average calorie content of 3 beers is more than 160 calories.
  
```{r}
```
  
  
4. The following code generates two ecdfs, one for calories of ales, one for calories of lagers. 
 
```{r}
Ales <- subset(BeerData, select=Calories, subset=Type=="Ale", drop=T)
Lagers <- subset(BeerData, select=Calories, subset=Type=="Lager", drop=T)
#plot.new()
plot.ecdf(Ales, col="red")
plot.ecdf(Lagers, col="blue", add=TRUE)
abline(v=165)
```

What percentage of ales in the data set have less than 165 calories? 

What percentage of lagers in the data set have less than 165 calories?


# Permutation test

Conduct a permutation test to determine whether there is a difference in the mean calorie content between ales and lagers. Your test should start with a statement of the formal hypotheses and finish with a conclusion.

Put each step in its own chunk and **clearly explain what each chunk is doing and what information the output provides.**

```{r}

```




```{r}

```



```{r}

```


```{r}

```



```{r}

```


(I put in several r-chunks.  You might use more, you might use fewer.)


# Critical reading

Below, I conduct a permutation test to test whether there was, on average, a difference in the annual snow fall in Colorado in the years 2019 and 2020.  Identify the mistake and fix it.

First, we load and briefly inspect the data:
```{r}
SnowData <- read.csv("https://www.dropbox.com/s/h8lkjdqoqui3bzw/TestDownload2.csv?dl=1")
head(SnowData)
```

The data shows annual snow fall recorded at 51 NOAA weather station in Colorado in 2019, and 2020. 


The formal statements of the hypotheses are:

$$H_0: \mu_{d_{2020}-d_{2019}}=0$$

$$H_A: \mu_{d_{2020}-d_{2019}}\neq 0$$

where $d_{2020}-d_{2019}$ represents the difference in snow fall by station. 

We'll compute the difference in snowfall at each station and compute the observed mean of differences:

```{r}
DiffAtStations<-SnowData$Snow2020-SnowData$Snow2019
Observed<-mean(DiffAtStations)
Observed
```

Next, we'll pool the data in order to find a permutation distribution:

```{r}
PooledData<- c(SnowData$Snow2020, SnowData$Snow2019)
```

Here is the loop that generates a permutation distribution:


```{r}
N=10^4
SampleDist<-numeric(N)
for (i in 1:N)
{index<-sample(102,51, replace=F)
SampleDist[i]<- mean(PooledData[index]-PooledData[-index])}
```



Finally, we compute the $p$-value for our observed statistic:

```{r}
2*min((sum(SampleDist >= Observed)+1)/(N+1),
(sum(SampleDist <= Observed)+1)/(N+1))
```

Since the $p$-value is small we reject the null hypothesis and conclude that the mean difference in snow depth between 2019 and 2020 was different.




**Your notes are below.  Do NOT delete the following chunk or your file may not knit.
```{r}
knitr::knit_exit()
```





STATS Terminology

â€¢	Observation: is a fact or figure we collect about a given variable. It can be expressed as a number or as a quality. An example of a number is the observation "25" for the age of a mother at the birth of her first child.

â€¢	Variable: A variable is an attribute that describes a person, place, thing, or idea. The value of the variable can "vary" from one entity to another.

â€¢	population: is the entire group of individuals you want to study, and a sample is a subset of that group.

â€¢	(random) sample: A simple random sample is a subset of a statistical population in which each member of the subset has an equal probability of being chosen. A simple random sample is meant to be an unbiased representation of a group. ... Random sampling is used in science to conduct randomized control tests or for blinded experiments.

â€¢	numeric variable: Numeric variables, as you might expect, have data values that are recognized as numbers. This means that they can be sorted numerically or entered into arithmetic calculations. When viewed in the Data View window, system-missing values for numeric variables will appear as a dot (i.e., â€œ.â€).

â€¢	factor variable: Factors are the variables that experimenterâ€™s control during an experiment in order to determine their effect on the response variable. ... Factors can be a categorical variable or based on a continuous variable but only use a limited number of values chosen by the experimenters.

â€¢	random variable: variable whose values depend on outcomes of a random phenomenon.

â€¢	independent and identically distributed: a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d. or iid or IID.

â€¢	sampling with replacement: When a sampling unit is drawn from a finite population and is returned to that population, after its characteristic(s) have been recorded, before the next unit is drawn, the sampling is said to be â€œwith replacementâ€.

â€¢	sampling without replacement: In sampling without replacement, each sample unit of the population has only one chance to be selected in the sample. For example, if one draws a simple random sample such that no unit occurs more than one time in the sample, the sample is drawn without replacement.

â€¢	parameter: A parameter is a quantitative characteristic of the population that youâ€™re interested in estimating or testing (such as a population mean or proportion).

â€¢	Statistic-A statistic is a quantitative characteristic of a sample that often helps estimate or test the population parameter (such as a sample mean or proportion).

â€¢	statistical inference- Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.

â€¢	observational study- use samples to draw conclusions about a population when the researchers do not control the treatment, or independent variable, that relates to the primary research question.

â€¢	experimental study: Experimental studies are ones where researchers introduce an intervention and study the effects. Experimental studies are usually randomized, meaning the subjects are grouped by chance. ... The researchers then study what happens to people in each group. Any difference in outcomes can then be linked to the intervention.

right skewed(long right tail)
left skewed (long left tail)

#---------------------------------------------------------------------------
The p-value is probability. 
If the p-value is small enough <  .05  then the null  can be rejected. 
When rejected then you have enough evidence that they are statistically different.
If you cannot reject the null then there is not enough evidence to say if the data is statistically different. 

Null hypothesis: difference in proportions is zero.  corresponds to no real effect. 
Alternative hypothesis: difference in proportions is not zero. a statement that there is a real effect.  

Means of difference vs difference of means 
Swaps data within brand. Changes calories with different brand. Calculate mean of difference. If it is unusual then there is a difference in brands. If its not unusual then its by chance. 

Permutation: (is a randomized test, resampling) in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under all possible rearrangements of the observed data points. 
You pool data to see if there is a difference in data. If there isnt unusual data then the data is by chance. 

ECDF's are empirical cumulative distribution they are useful for comparing two distributions. They are step functions 


# QQPlots ----------------------------------------------------------------------------------------------------

```{r}
library(ggplot2)
library(tidyverse)
Spruce <- read.csv("https://sites.google.com/site/chiharahesterberg/data2/Spruce.csv")
```

Let's look at the data from the Spruce study. In particular, we'll look at the distribution of the Height after 5 years:

```{r}
summary(Spruce)
hist(Spruce$Height5, freq=F)
mean(Spruce$Height5)
sd(Spruce$Height5)
```

One of the most common assumptions we will make about a population is that it is **normally distributed.**

So, we need to know:

1) What is a normal distribution.
2) How do we know if a population is normally distributed?

A normal distriubtion (for a continuous random variable) has a graph that looks like the one below.  It has two **parameters**: the mean and the standard deviation.

Remember, a *pdf* is the function $f$ such that $$P(a\leq X \leq b)=\int_a^b f(t) \ dt.$$

For a normal distribution $$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$ where $\mu$ is the mean, and $\sigma$ is the standard deviation.

Below is a normal distribution with $\mu=4$ and $\sigma=2$:

```{r}
m=4
s=2
f <- dnorm(m,s)
base <- ggplot(data.frame(x = c(m-3*s, m+3*s)), aes(x))
base + stat_function(fun = dnorm, args=list(mean=m, sd=s))
```

Shown are both a histogram of the sample as well as the normal distribution with the same mean and standard deviation of the data.

```{r}
m=mean(Spruce$Height5)
s=sd(Spruce$Height5)
ggplot(data.frame(Spruce$Height5), aes(x = Spruce$Height5)) +
  geom_histogram(bins=10, aes(y=..density..))+
  stat_function(fun = dnorm,  args=list(mean=m, sd=s), color = "green")
```

The shapes of the histogram and of the normal distribution are fairly close.  It's probably fair to assume that the height of spruce trees is normally distributed.

A tool, called **qqplots**, allows us to investigate this question a little better.

We ask the following question: "What value, $q$, gives $P(X \leq q)=0.1$?  The answer is called a quantile.  Or, if converted to a percentage, a percentile.

We compute these quantiles for various values: 0.1, 0.2, etc.  Call these answers $q_1, q_2, q_3, \dots$.

We do this two ways:

1) for the theoretical *standard* normal distribution, which has $\mu=0$ and $\sigma=1$.

2) for the sample data


From the *standard* normal distribution (theoretical)
```{r}
qnorm(.1)
```


That is, $P(X \leq -1.281552)=0.1$ is $X$ is a random variable with a standard normal distribution.

Then we do the same thing for the data. How much of data lies in the first 10% of the data, the first 20% of the data, etc.  (The data isn't continuous so we won't do this calculation, we'll let R figure it out....)  Call these answers $x_1, x_2, x_3, \dots$

Now, plot the points $(q_1, x_1), (q_2, x_2), \dots$.

The  closer these points come to being on a straight line the more *normal* the distrubution.

```{r}
plot.new()
qqnorm(Spruce$Height5)
qqline(Spruce$Height5)
```

```{r}
FlightData <-  read.csv("https://sites.google.com/site/chiharahesterberg/data2/FlightDelays.csv")
#head(FlightData)
```

By way of comparison, let's look at a qqplot for delay times from the Flight Delay data set
```{r}
plot.new()
qqnorm(FlightData$Delay)
qqline(FlightData$Delay)
```

The plotted points deviate far from the line due to the fact that the data is skewed right.

# 2.7-----------------------------------------------------------------------------------------------------------------------------------------------------
---

```{r}
spruce <- read.csv("https://sites.google.com/site/chiharahesterberg/data2/Spruce.csv")
#head(spruce)
```

Part a. Compute the numeric summareis for the height changes (Ht.Change) of the seedlings
```{r}
summary(spruce$Ht.change)

```
Part b. Create a histogram adn normal quantile plot for the height changes of the seedlings. Is the distribution approximatley normal?
```{r}

hist(spruce$Ht.change)
qqnorm(spruce$Ht.change)
qqline(spruce$Ht.change)

```

The histogram is a little skewed right and there are two peaks in the histogram. 
The q-q plot is a little skewed off the ideal line but the plot is approximately normal. 

Part c. Create a boxplot to compare the distribution of the change in diameter of the seedlings (Di.Change) grouped by whether or not they were in fertilized plots.  
```{r}
boxplot(spruce$Di.change~spruce$Fertilizer)
```

Part d. Use the tapply command to find the numeric summaries of the diameter changes for the two levels of fertilziation 
```{r}
tapply(spruce$Di.change,spruce$Fertilizer,summary)
tapply(spruce$Di.change,spruce$Fertilizer,sd)
```

part e. Create a scaller plot of the height changes against the diameter changes, and describe the relationships. 
```{r}
plot(spruce$Ht.change,spruce$Di.change, main='Scatterplot')

```

Describe the relationship: As the change of height increases, the change in diameter also increases. This scatterplot is a positive linear graph. 


#ECDF--------------------------------------------------------------------------------------------------------------------------------


Recall:

A function $f$ such that $$P(X \leq x)=\int_{-\infty}^x f(t) \ dt$$ is the pdf (probability distribution function).

The function $$F(x)=P(X \leq x)$$ itself is called the cummulative distribution function (cdf).  Notice $$F^\prime(x) = \text{pdf}(x)$$

So, the pdf and cdf convey similar information. Sometimes it's convenient to look at the pdf, sometimes the cdf.

The graph on the front cover of the book is a cdf. It show exactly the two things we're interested in.  The solid curve can be thought of as the theoretical cdf, while the dotted curve is the cdf determined from the sample data.  

Since the cdf is defined as a probability, its values are always beteen 0 and 1.  Also, its graph always has the same shape.  Where and how steeply the graph goes from 0 to 1 changes.

The horizontal axes of a cdf graph has units of the data (time, number of people, etc.), the vertical axes is probability/percentage.

Let's look at an example.  We'll consider the birth weight of babies of smoking vs non-smoking mothers.

```{r}
BirthsData <- read.csv("https://sites.google.com/site/chiharahesterberg/data2/TXBirths2004.csv")

```

```{r}
head(BirthsData)
```

First, we need to prepare the data:

```{r}
smoker <- subset(BirthsData, select=Weight, subset=Smoker=="Yes", drop=T)
head(smoker)
nonsmoker <- subset(BirthsData, select=Weight, subset=Smoker=="No", drop=T)
head(nonsmoker)
```

First, let's look at the histogram (corresponds to pdf) for each group:

```{r}
hist(smoker, freq=F)
hist(nonsmoker, freq=F)
```

Both are unimodal and roughly normal, but the nonsmoker distribution is skewed left.

Now, let's look at the cdf for the sample data, this is called the **empircal cummulative distribution function** (ecdf)

```{r}
plot.new()
plot.ecdf(smoker, col="red")
plot.ecdf(nonsmoker, col="blue", add=TRUE)
abline(v=3000)
```

$$F_n(x)=\frac{\text{number of values } \leq x}{\text{sample size}}$$

Let's think about babies with a birth weight of 3000 grams or less. Roughly: 

* about 20% of babies of non-smoking mothers had a birth weight of 3000 grams or less;  

* about 40% of babies of smoking mothers had a birth weight of 3000 grams or less. 


You can obtain the same information from the pdfs, but it's much easier to see here.

* In cdfs the probability is on the vertical axes.  
* In pdfs the probability is an area under the curve.

(remember, be careful about making scientific conclusions: this is an observational study, not an experimental one)

**Skewness and Kurtosis**

*Skewness* and *Kurtosis* describe how far a distribution from being normal. 

*Skewness* reflects how symmetric (or nonsymmetric) a distribution is. 

* a positive number means skewed right
* a negative number means skewed left
* the farther the number is from zero, the more skewed it is

*Kurtosis* reflects high peaks (higer than a normal distribution) and longer tails (again, as compared to a normal distribution)

* For now, we'll content ourselves with: the farther the Kurtosis is from 0, the more "un" normal the distribution

Before you run the chunk below you need to load the "moments" package.  Do this in the **console** window by running: install.packages("moments").

(You'll probably only need to do this once on your laptop.)


```{r}
library(moments) 
skewness(smoker)
skewness(nonsmoker)
kurtosis(smoker)
kurtosis(nonsmoker)
```

So, the non-smoker distribution is more skewed (left) than the smoker distribution.  

The nonsmoker distribution also has a shape further from normal than the smoker distribution.

# 2.6 -----------------------------------------------------------------------------------------------------------------------------------------------------------------

part a. Create a table and bar chart of the Recid variable. 
```{r}
Recidivism <- read.csv("https://sites.google.com/site/chiharahesterberg/data2/Recidivism.csv")
head(Recidivism)
table(Recidivism$Recid)
barplot(table(Recidivism$Recid))

```

part b. create a contingency table summarizing the relationship between recidivism by age 25. Of those under 25 years of age what is the proportion? 
```{r}
table(Recidivism$Recid,Recidivism$Age25)
prop.table(table(Recidivism$Recid,Recidivism$Age25),2)

```

For people under 25 years of age, 36% were sent back to prison. 30% of people over 25 years were returned to prison.

part c. Create side by side boxplots of the number of days to recidivism grouped by type of violation, and give three comparative statements about the distributions.
```{r}
boxplot(Recidivism$Days~Recidivism$Offense)
```

The two plots are similar. The interquartile is about the same, the spread about the same size, and there are no outliers. The medians are also close. 

part d. Use the quantile command to obtain the quareitles of the number of days to recidivism. Since there are missing values (NA) for those released offenders who had not recideivated, you will need to add the argument na.rm=TRUE. 
```{r}

quantile(Recidivism$Days, na.rm=TRUE)
```

part e. Create ecdfs of days to recidivism for those under 25 years of age and those 25 years or older approx what proportion in each age group were sent back to prison 400 days after release?
```{r}
under25 <- subset(Recidivism, select=Days, subset=Age25=="Under 25", drop=T)

over25 <- subset(Recidivism, select=Days, subset=Age25=="Over 25", drop=T)

plot.ecdf(under25, col='red')
plot.ecdf(over25, col='blue', add=TRUE)

```

What proportion in each age group were sent back to prison 400 days after release?

Under 25: about 40% of people under 25 were sent back to prison 400 days after release. 
over 25: about 50% of people over 25 were sent back to prison 400 days after release. 

Homework Add on 
```{r}
offenseF <- subset(Recidivism, select=Days, subset=Offense=="Felony", drop=T)

offenseM <- subset(Recidivism, select=Days, subset=Offense=="Misdemeanor", drop=T)

plot.ecdf(offenseF, col='red')
plot.ecdf(offenseM, col='blue', add=TRUE)
abline(v=280)
```

```{r}
boxplot(Recidivism$Days~Recidivism$Race, na.rm=TRUE)

```

Give two groups that appear to have similar distributions: The boxes over black-Hispanic and white-non Hispanic (box 2 and box 4 from the right) have similar spread, median, and interquartile box. 

Give two groups for which it might be true that the mean number of days to recidivism are statistically different: 
The box plot for white is smaller than the box plot for the Native-Hispanic resulting in having statistically different means. The white group also has a low median bar with very little spread. 

# Chapter 3 Hypothesis Testing ----------------------------------------------------------------------------------------------------------------------------------------

Idea of hypothesis testing
-----
The example shows times it takes mice to run through a maze.  Does the data from this one iteration of the experiment provide evidence that the drug has an effect on ability to run through the maze quickly?  Or, could this data simply be a result of chance.  Eg. maybe, by chance, slow mice were in the group that received the drug while fast mice were in the control group.

If we restrict our attention to these 6 times, here are all the ways three of the times could have been (but weren't) placed in the drug group:
```{r}
times=c(30, 25, 20, 18, 21, 22)
druggroup <- combn(times,3)
druggroup

```

Notice, the first column is the *observed* outcome. We'll find the mean time of this group as the *observed* mean time.

```{r}
druggroup[,1]
mean(druggroup[,1])
```

In general if I choose $k$ items from $n$ total items there are "n choose k" ways to do this:

```{r}
choose(6,3)
```


Here are the corresponding times in the control group.  That is, once I choose 3 of the times for the drug group, the other three times would be in the control group (no drug).

```{r}
controlgroup <- matrix(data=0, nrow=3, ncol=choose(6,3))
for (i in 1:20)
  for (j in 1:3)
    controlgroup[j,i]=setdiff(times, druggroup[,i])[j]
controlgroup
```

We want to, for each possibility, one at a time, compare the mean time from the drug group to the mean time from the control group.

The first column corresponds to the data from our experiment,  the average times are, and the difference is:
```{r}
druggroup[,1]
mean(druggroup[,1])

controlgroup[,1]
#cat("mean time of control group", mean(controlgroup[,1]))

mean(controlgroup[,1])

mean(druggroup[,1])-mean(controlgroup[,1])
```

So in this case the drug group took longer (on average) to complete the maze than the control group.

We want to compare the mean time of the drug group to the mean time of the control group for all the possibilities:

```{r}
colMeans(druggroup)-colMeans(controlgroup)
```

In how many of the possible groupings is the difference in mean times as extreme or more extreme than our actual experiment?

The following code asks case by case whether the mean time to completion for any possibility is $\geq$ the mean time for completion of the observed selection.

In R it's easy to count because R treats the "sum" of True and False values as 1 and 0 respectively. 

```{r}
colMeans(druggroup)-colMeans(controlgroup)>= mean(druggroup[,1])-mean(controlgroup[,1])

sum(colMeans(druggroup)-colMeans(controlgroup)>= mean(druggroup[,1])-mean(controlgroup[,1]))
```

So the probability that chance would give a difference *at least as large* as our observed experiment is

```{r}
3/20
```

To conclude that the drug DID have an effect on times we'd want this outcome to be *very* unlikley.  While 0.15 is *small* it isn't usually considered small enough.  

So, this difference of mean time is *not*  *statistically signficant*. We cannot conclude (statistically) that the drug has an effect of maze time.

What is *small enough*?  Good question.  To some extent it depends on discipline.  Biologist, social scientists, etc. have different standards. Typical values are 0.01 or 0.05.  

# Exercise 3.6----------------------------------------------------------------------------------------------------------------------------------------------

Null Hypothesis two-sided test statement:

$$H_A: \mu_{UA}-\mu_{AA}\neq 0.$$
Part a. Compute the proption of times that each carrier's flights was delayed more than 20 min. Conduct a two sided test to see if the difference in these proptions is statistically significant. 
```{r}
delay <- read.csv("https://sites.google.com/site/chiharahesterberg/data2/FlightDelays.csv") 
#head(delay)

UAdelays <- subset(delay, select=Delay, subset=Carrier=="UA", drop=T)
AAdelays <- subset(delay, select=Delay, subset=Carrier=="AA", drop=T)


UA20 <- subset(UAdelays, subset=UAdelays>20)
AA20 <- subset(AAdelays, subset=AAdelays>20)

propAA <-length(AA20)/length(AAdelays)
propAA
propUA <-length(UA20)/length(UAdelays)
propUA


observed <- propAA-propUA
observed
```


creating a new sampling pool 
```{r}
alldelays <- delay$Delay
poolsize <- length(alldelays)
#poolsize
GroupUA <- length(UAdelays)
#GroupUA
GroupAA <- length(AAdelays)
#GroupAA
```


```{r}
N <- 10^4-1
result <- numeric(N)
for (i in 1:N)
{index <- sample(poolsize,size=GroupUA, replace=F)
result[i] <- (sum(alldelays[index]>20)/GroupUA)-(sum(alldelays[-index]>20)/GroupAA)}
```


```{r}
hist(result)
```

Computes probability
```{r}

2*min((sum(result >= observed)+1)/(N+1),(sum(result <= observed)+1)/(N+1)) #p-value
```
p value is really small between the UA and AA flight delays so the null hypothesis is rejected. Since the null hypothesis is rejected then the data is statistically significant. 

part b. Compute the variance in flight delaylenghts for each carrier. Conduct a test to see if the variave for UNited airlines differes from that of american airlines.
Null Hypothesis two-sided test statement:

$$H_0: var_{UA}/ var_{AA}= 1.$$
```{r}
var(UAdelays)
var(AAdelays)
observe2 <- var(UAdelays)/var(AAdelays)
observe2
```

resampling 
```{r}
N <- 10^4-1
result2 <- numeric(N)
for (i in 1:N)
{index <- sample(poolsize, size=GroupUA, replace=F)
result2[i] <- var(alldelays[index])/var(alldelays[-index])}
hist(result2)
```
```{r}
2*min((sum(result2 >= observe2)+1)/(N+1), (sum(result2 <= observe2)+1)/(N+1)) #p-value
```
The variance between American airlines and united airlines is not small enough so you cannot reject the null hypothesis. Since the null hypothesis is not rejected then there is not enough evidence to conclude if there is a difference or not. 

# 3.9------------------------------------------------------------------------------------------------------------------------------------------------------


Hypothesis test statements:

$$H_0: p_{May}-p_{June}=0$$

$$H_A: p_{May}-p_{June}\neq 0$$

Load the data. Create two subsets, one each for May and June.

```{r}
library(resampledata)
delays <- FlightDelays
Maydelays <- subset(delays, select=Delay, subset=Month=="May", drop=T)
Junedelays <- subset(delays, select=Delay, subset=Month=="June", drop=T)

```

Look at the three vectors and their sizes to make sure they seem like the right thing.

```{r}
length(Maydelays)
length(Junedelays)
length(delays$Delay)
```


Create subsets of delays more than 20 minutes.  Again look at the vectors and their sizes to make sure they are the right thing.

```{r}
May20 <- subset(Maydelays, subset=Maydelays>20)
length(May20)
length(Maydelays)
June20 <- subset(Junedelays, subset=Junedelays>20)
#June20
```

Computation of proportions:

```{r}
length(May20)/length(Maydelays)
length(June20)/length(Junedelays)
```

Or, another way to do this:

Compute the proportion of delays more than 20 minutes in each month:
```{r}
PropMay <- (sum(Maydelays>20))/(length(Maydelays))
PropMay
PropJune <- (sum(Junedelays>20))/(length(Junedelays))
PropJune
```

```{r}
sum(Maydelays>20)
```

Look at the vectors that are in the lines of code above.  To see that they are the "right" thing and that I "count" them in the correct manner.

Compute the *observed* difference in proportions:
```{r}
observed <- PropMay-PropJune
observed
```

```{r}
boxplot(delays$Delay~delays$Month)
```

Pool the data and record the size of 1) the pooled data, and 2) the size of the two groups:

```{r}
alldelays <- delays$Delay
poolsize <- length(alldelays)
poolsize
GroupOneSize <- length(Maydelays)
GroupOneSize
GroupTwoSize <- length(Junedelays)
GroupTwoSize
```

Run a resampling permutation test:

```{r}
N <- 10^6-1
result <- numeric(N)
for (i in 1:N)
{index <- sample(poolsize,size=GroupOneSize, replace=F)
result[i] <- (sum(alldelays[index]>20)/GroupOneSize)-(sum(alldelays[-index]>20)/GroupTwoSize)
} 

```

Look at results of the permutation test:

```{r}
hist(result)
```

Compute probability of an outcome at least as extreme as the observed outcome:

```{r}
2*min((sum(result >= observed)+1)/(N+1),
(sum(result <= observed)+1)/(N+1))
```


At the 0.05 level we can conclude there is a significant differnce in the proportion of delays over 20 minutes in the months of May vs. June.

At the 0.01 level we cannot make this conclusion.

# Matched Pairs -------------------------------------------------------------------------------------------------------------------------------------------------

```{r}
library(resampledata)
head(IceCream)
```

If we compare calories of vanilla ice cream vs calories of chocolate ice cream we need to keep the data matched according to brand.  It doesn't make sense to pool the data the same way we did for other examples.  Instead, we will randomly decide to swap data points *within the same matched pair* when we resample.  For example if we randomly choose Ben & Jerry's to swap, we'll record vanilla calories at 260 and chocolate calories as 240.

First, let's compute the observed difference in mean calories of vanilla ice cream vs. chocolate ice cream. We want to make sure we compute the differences in the matched pairs:

```{r}
DiffInPairs <- IceCream$VanillaCalories-IceCream$ChocolateCalories
DiffInPairs
observed <- mean(DiffInPairs)
observed

```


```{r}
summary(DiffInPairs)
sd(DiffInPairs)
```

So, in the observed outcome chocolate ice cream has more calories on average.

To swap values, we'll randomly compute:

Chocolate - Vanilla 

instead of 

Vanilla - Chocolate.

This is easy to do: we pick random rows from the data and multiply the difference by $-1$.

First, we need to know how many observations there are in the data:

```{r}
poolsize <- length(IceCream$Brand)
poolsize
```
```{r}
```


Next, we create a vector of $\pm 1$:

```{r}
sample(c(-1,1), poolsize, replace=T)
```

We're ready to conduct the permutation test for *matched pairs*:

```{r}
N <- 10^4-1
result <- numeric(N)
for (i in 1:N)
{Sign <- sample(c(-1,1), poolsize, replace=T) #random swapping assignment
  Diff <- Sign*DiffInPairs #compute differences within matched pairs
result[i] <- mean(Diff) #assign the mean of the differences to the result vector
} 
```


Look at result:
```{r}
hist(result)
```

Compute $p$-value:

```{r}
min((sum(result >= observed)+1)/(N+1),
(sum(result <= observed)+1)/(N+1))
```

So, since the $p$-value is small we have evidence to reject the null hypothesis and conclude that there is a difference in calories between vanilla and chocolate ice cream.

# 3.15 Groceries ------------------------------------------------------------------------------------------------------------------------------------------------------

```{r}
library(resampledata)
#head(Groceries)
```
Part A. inspect the data set then explain why this is an example of matched pairs data.  

This is matched pair data because you want to know the difference in prices by the products so you cannot mix the retailers or products together. YOu only want to swap the prices for each product while keeping the retailers the same. You match the pairs. 


Part B. compute summary statistics of the prices fro each store
```{r}
summary(Groceries$Target)
summary(Groceries$Walmart)
```

Part C. conduct a permutation test to determine whether or not there is a difference in the mean prices. 
```{r}
diffPairs = Groceries$Target - Groceries$Walmart
diffPairs
observed <- mean(diffPairs)
observed

poolsize<-length(Groceries$Product)
poolsize
sample(c(-1,1), poolsize, replace=T)

N <- 10^4-1
result <- numeric(N)
for (i in 1:N)
{Sign <- sample(c(-1,1), poolsize, replace=T) #random swapping assignment
  Diff <- Sign*diffPairs #compute differences within matched pairs
result[i] <- mean(Diff)} #assign the mean of the differences to the result vector


2*min((sum(result >= observed)+1)/(N+1),
(sum(result <= observed)+1)/(N+1))
```

The pvalue isn't small enough to reject the null hypothesis. Therefore there is not enough evidence to conclude anything about the data. 


Part D. create a histogram of the difference in prices, What is unusual about Quaker oats life cereal?
```{r}
hist(diffPairs)
#hist(result)
```

The Quaker oats life cereal is an outlier. The price for the quakers oat cereal from walmart is $6.01 which is about twice the price from target. 


Part E. Redo the hypotheis test without this observation. Do you reach the same concludion? 
```{r}
new_diff <- diffPairs[diffPairs>-2]
new_diff

observed2 <- mean(new_diff)
observed2

poolsize<-length(new_diff)
poolsize
sample(c(-1,1), poolsize, replace=T)

N <- 10^4-1
result2 <- numeric(N)
for (i in 1:N)
{Sign <- sample(c(-1,1), poolsize, replace=T) #random swapping assignment
Diff2 <- Sign*new_diff #compute differences within matched pairs
result[i] <- mean(Diff2)} #assign the mean of the differences to the result vector


2*min((sum(result >= observed2)+1)/(N+1),
(sum(result <= observed)+1)/(N+1))

hist(new_diff)
hist(result)
```

The pvalue is a lot smaller than without quakers oats cereal included. The pvalue is small enough to reject the null hypothesis. This means there is a statistical difference in the price difference between the product and retailer. 

# Sampling Distribution -------------------------------------------------------------------------------------------------------------------------------------------

When we did a permutation test we resampled our sample data in order to generate a permutation distribution.  We looked at the observed statistic's (difference of means, difference of proportion, etc.) location in the distribution in order to determine whether or not the observed statsitic was *statistically signficant*.

This is *one* way to determine a distribution. 

The following illustrates the *idea* behind finding a distribution in a different way.  Keep in mind that what we're doing *illustates* the idea; it is not part of the test.

The following data set contains finish times from the Chicago Marathon.  I don't know what subset of times this is. But, we are going to pretend this is the *population.*  That is, pretend it is *all* times, or even *all* marathon times. It would be difficult to collect *all* marathon times, but not difficult to sample marathon times. We could compute the mean time from our sample.  But, that raises the question if our sample was a *good* sample. Maybe the next sample produces a different mean time, etc.  

Repeating this -  sample, and calculate the mean time - produces a *sampling distribution*.  This will be a distribution against which we can compare an oberserved statistic.


Here is the data set:

```{r}
library(resampledata)
head(ChiMarathonMen)
```

We're pretending, but just so we sort of know the setting:

```{r}
summary(ChiMarathonMen$FinishMin)
```

These are pretty fast times, but not world class fast.

Here is the distribution of the *population*:

```{r}
hist(ChiMarathonMen$FinishMin)
```

Keep in mind, would probably couldn't *know* this for a population, but it's not too much of stretch that we'd assume something like this: there are only a few *really* fast time, then things are fairly *uniform*.

We are going to choose a sample of 6 times, calculate the mean, store that result, repeat...

Here is one pass of the simulation:

```{r}
mysample <- sample(ChiMarathonMen$FinishMin, size=6,replace=F)
mean(mysample)
```

Here is the simulation:

```{r}
N=10^4
Xbar <- numeric(N)
for (i in 1:N)
  {mysample <- sample(ChiMarathonMen$FinishMin, size=6,replace=F)
  Xbar[i] <- mean(mysample)}
```

Here is the *sampling distribution*.  It is the sampling distribution of the mean of a sample of size 6.  This is often denoted $\overline{X}$.  $\overline{X}$ is the random variable.

```{r}
hist(Xbar)
```

The mean and standard deviation for the population - that is the *parameters* - are:

```{r}
mean(ChiMarathonMen$FinishMin)
sd(ChiMarathonMen$FinishMin)
```

The mean and *standard error*  (the standard deviation of a sample is called the *standard error*) of the *sampling distribution* - that is the *statistics* - are:

```{r}
mean(Xbar)
sd(Xbar)
```

Notice:

* The two mean values are very close to one another.
* The two standard deviations are not, but:

```{r}
sd(ChiMarathonMen$FinishMin)/sqrt(6)
```

is pretty close to the standard error.

Let's try a different statistic. We'll let $X$ be the fastest time in a sample of 6 times:

```{r}
mysample <- sample(ChiMarathonMen$FinishMin, size=6,replace=F)
min(mysample)
```

Again, we repeat this process many times:

```{r}
N=10^4
Xmin <- numeric(N)
for (i in 1:N)
  {mysample <- sample(ChiMarathonMen$FinishMin, size=6,replace=F)
  Xmin[i] <- min(mysample)}
```

Here is the sampling distribution for $X_{min}$:
```{r}
hist(Xmin)
```

The mean and standard deviation of the population are:

```{r}
mean(ChiMarathonMen$FinishMin)
sd(ChiMarathonMen$FinishMin)
```

The mean and standard error of the sampling distribution are:

```{r}
mean(Xmin)
sd(Xmin)
```

It's not so clear what (if anything) is happening here.  One point here is that the two statistics: sample mean vs sample min must be treated differently.  

# Binomial Distribution---------------------------------------------------------------------------------------------------------------------------------------
---

According to the CDC the positivity rate for coronavirus in La Plata county on Wednesday, 24-Feb was $2.15\%$.  Suppose 15 people test on Friday.  Let $X$ be a discrete random variable that is the number of people (of those 15) who test positive.  We can ask questions like:

* What is the probability that 2 (exactly) people test positive? $$P(X=2)$$

* What is the probability that 3 or fewer people test postive? $$P(X \leq 3)$$.

The probability that one person tests positive is 

```{r}
postest <- 0.0215
```

We are making two very important assumptions:

* The probability for each person who tests is the **same**.
* Each person's test is **independent** of other tests.

The probabilty that one person test negative is
```{r}
negtest <- 1-postest
```

There are many ways 2 people could test positive:

* the first two test postive, the remaining 13 negative
* the first and third test positive, the remainig 13 negative
* etc.
* How many different combinations are there?  $$15 \choose 2:$$

```{r}
choose(15, 2)
```

Every one of these ways has a probability of $$(0.0215)^2 \cdot (1-0.0215)^{13}$$ of happening. So, the probability of exactly two people out of 15 testing positive is:

```{r}
choose(15, 2)*(postest)^2*(negtest)^13
```

The random variable for this situation has a *binomial distribution*.

Remember, this is a discrete random variable, not a continuous random variable.

The usual language for binomial distributions is that one possibility (testing positive) is called a *success*, the other possibility (testing negative) is called a *failure*.  There are only *2 options*.  Other examples: it rains today (or not),  passing a statistics exam (or not), etc.

Here is the binomial distribution for this random variable:


(It's maybe a little easier to see with `type='b'`, just remember the random variable is discrete.)

```{r}
plot(dbinom(0:15, 15, postest), type='h')
```


I found a better way to plot:

```{r}
barplot(dbinom(0:15, 15, postest), names.arg=0:15)
```


For different probablities of success we obtain different shaped distributions:

```{r}
plot(dbinom(1:15, 15, .2), type='b')
```

The probability that 3 or fewer people test positive (so 0, 1, 2, or 3 people test positive) is:



$${15\choose 0} s^0 f^{15}+ {15 \choose 1} s^1 f^{14}+ {15\choose 2} s^2 f^{13}+ {15 \choose 3} s^3 f^{12}$$
Where $s$ is the probability of "success" and $f$ is the probability of "failure."

```{r}
choose(15,0)*postest^0*negtest^15+choose(15,1)*postest^1*negtest^14+choose(15,2)*postest^2*negtest^13+choose(15, 3)*postest^3*negtest^12
```
There are 4 basic R-functions associated with a distribution. 

* d-*dist* we saw above when we plotted the distribution.  It is the *pdf*
* p-*dist* which computes the area in the lower tail, i.e. $P(X \leq x)$. It is the *cdf*. Be *very* careful - the $\leq$ makes a difference!
* q-*dist* which computes the value $q$ such that $P(X \leq q)= \text{ the prob. you send}$
* r-*dist* which generates a random sample from the given distribution.  This is useful for running simulations.

The calls for different distributions are each a bit different depending on the parameters the distribution takes.

Here is the computation we did above:

```{r}
pbinom(3,15,postest)
```

This particular example isn't great for illustrating `qbinom`...we'll come back to it. 

But `qbinom` is the inverse of `pbinom`: you give it a probability, $p$, and it gives you back (in this case) the number of positive tests you'll have with probability $p$.



Finally, if we want to generate a random sample from this distribution:

```{r}
rbinom(100, 15, postest)
```

If you plot these data in a histogram - you have a *sampling distribution* from a binomials distributed population.


# Exercise 4.14 -----------------------------------------------------------------------------------------------------------------------------------------------------

```{r}
Diploma <- .286 
noDiploma <-1-Diploma
prob <- pbinom(230,800,Diploma)-pbinom(219,800,Diploma)
prob

plot(dbinom(0:800,800,Diploma), type='b')

```

There is a 32% probability that between 220 and 230 of the adult US population have high school diplomas. 


Another Problem

```{r}
one6 <-1/6
one <-pbinom(0,6,one6)
1-one

two6 <- 2/12
two<-pbinom(1,12,two6)
1-two

three6 <- 3/18
three<-pbinom(2,18,three6)
1-three



plot(dbinom(0:15, 6, one), type='b')
plot(dbinom(0:15, 12, two), type='b')
plot(dbinom(0:18, 18, three), type='b')

```

Rolling 6 fair dice and having at least one six appear has the greatest probability. 
 


